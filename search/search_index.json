{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Intermediate-Advanced Shell for Bioinformatics \u00b6 Setup and Download Data UNIX, Shell, Linux & Bash UNIX Shell Basics & Recap Inspecting and Manipulating Text Data with UNIX Tools - Part 1 Inspecting and Manipulating Text Data with UNIX Tools - Part 2","title":"Intermediate-Advanced Shell for Bioinformatics"},{"location":"#intermediate-advanced-shell-for-bioinformatics","text":"Setup and Download Data UNIX, Shell, Linux & Bash UNIX Shell Basics & Recap Inspecting and Manipulating Text Data with UNIX Tools - Part 1 Inspecting and Manipulating Text Data with UNIX Tools - Part 2","title":"Intermediate-Advanced Shell for Bioinformatics"},{"location":"0_setup_download/","text":"Setup & Download Data \u00b6 Setup \u00b6 Remote \u00b6 Local \u00b6 Local host setup - Windows, MacOS & Linux Windows Hosts MacOS Linux Install either Git for Windows from https://git-scm.com/download/win OR MobaXterm Home ( Portable or Installer edition) from https://mobaxterm.mobatek.net/download-home-edition.html Portable edition does not require administrative privileges Native terminal client is sufficient. It might not comes with wget download data via command line (can be installed with $ brew install wget ) However, it is not required as we provide a direct link to download data in .zip format Native terminal client is sufficient. bioawk install on all hosts One of the tools used in this workshop is bioawk which is not native to any terminal clients. Installing it on MacOS and Linux can be done with $ brew install bioawk & $ sudo apt install bioawk , respectively.Windows hosts might have to do it via conda according to these instructions . However, this will require a prior install of Anaconda Or Miniconda Download data \u00b6 Download via Web Browser Data can be downloaded directly from this link which will download bashfbio_v1.0_data.tar.gz to Downloads directory If the above link fails, try this alternative .zip Download with wget from a terminal client $ wget -c https://github.com/GenomicsAotearoa/shell-for-bioinformatics/releases/download/v1.0/data_shell4b.tar.gz OR $ wget -c https://github.com/GenomicsAotearoa/bash-for-bioinformatics/releases/download/v1.0rc1/data_shell4b.zip \u00ab Home 1 - Unix Shell and Bash \u00bb","title":"Setup & Download Data"},{"location":"0_setup_download/#setup-download-data","text":"","title":"Setup &amp; Download Data"},{"location":"0_setup_download/#setup","text":"","title":"Setup"},{"location":"0_setup_download/#remote","text":"","title":"Remote"},{"location":"0_setup_download/#local","text":"Local host setup - Windows, MacOS & Linux Windows Hosts MacOS Linux Install either Git for Windows from https://git-scm.com/download/win OR MobaXterm Home ( Portable or Installer edition) from https://mobaxterm.mobatek.net/download-home-edition.html Portable edition does not require administrative privileges Native terminal client is sufficient. It might not comes with wget download data via command line (can be installed with $ brew install wget ) However, it is not required as we provide a direct link to download data in .zip format Native terminal client is sufficient. bioawk install on all hosts One of the tools used in this workshop is bioawk which is not native to any terminal clients. Installing it on MacOS and Linux can be done with $ brew install bioawk & $ sudo apt install bioawk , respectively.Windows hosts might have to do it via conda according to these instructions . However, this will require a prior install of Anaconda Or Miniconda","title":"Local"},{"location":"0_setup_download/#download-data","text":"Download via Web Browser Data can be downloaded directly from this link which will download bashfbio_v1.0_data.tar.gz to Downloads directory If the above link fails, try this alternative .zip Download with wget from a terminal client $ wget -c https://github.com/GenomicsAotearoa/shell-for-bioinformatics/releases/download/v1.0/data_shell4b.tar.gz OR $ wget -c https://github.com/GenomicsAotearoa/bash-for-bioinformatics/releases/download/v1.0rc1/data_shell4b.zip \u00ab Home 1 - Unix Shell and Bash \u00bb","title":"Download data"},{"location":"1_introduction/","text":"What is a Unix Shell and Bash \u00b6 \u00ab Setup 3 - Shell Basics and recap \u00bb The UNIX operating system \u00b6 Unix is a multi-user operating system which allows more than one person to use the computer resources at a time. It was originally designed as a time-sharing system to serve several users simultaneous. Unix allows direct communication with the computer via a terminal, hence being very interactive and giving the user direct control over the computer resources. Unix also gives users the ability to share data and programs among one another. Unix is a generic operating system which takes full advantage of all available hardware such as 32-bit processor chips, expanded memory, and large, fast hard drives. Since Unix is written in a machine-independent language (C/C++) it is portable to many different types of machines including PC's. Therefore, Unix can be adapted to meet special requirements.The UNIX operating system is made up of three parts; the kernel, the shell and the programs. On Unix philosophy \u201cAlthough that philosophy can\u2019t be written down in a single sentence, as its heart is the idea that the power of a system comes more from the relationships among programs than from the programs themselves. Many UNIX programs do quite trivial things in isolation, but, combined with other programs, become general and useful tools.\u201d \u2013 Brian Kernighan & Rob Pike UNIX vs. Linux \u00b6 Linux is not Unix, but it is a \"Unix-like\" operating system. Linux system is derived from Unix and it is a continuation of the basis of Unix design. Linux distributions are the most famous and healthiest example of the direct Unix derivatives. BSD (Berkley Software Distribution) is also an example of a Unix derivative. Unix-like & a bit more on Linux A Unix-like OS (also called as UN*X or *nix) is the one that works in a way similar to Unix systems, however, it is not necessary that they conform to Single UNIX Specification (SUS) or similar POSIX (Portable Operating System Interface) standard. SUS is a standard which is required to be met for any OS to qualify for using \u2018UNIX\u2019 trademark. This trademark is granted by \u2018The Open Group\u2019 Few Examples of currently registered UNIX systems include macOS, Solaris, and AIX. If we consider the POSIX system, then Linux can be regarded as Unix-like OS. Linux is just the kernel and not the complete OS. This Linux kernel is generally packaged in Linux distributions which thereby makes it a complete OS. A Kernel is a computer program that is the heart and core of an Operating System. Since the Operating System has control over the system so, the Kernel also has control over everything in the system. It is the most important part of an Operating System. Whenever a system starts, the Kernel is the first program that is loaded after the bootloader because the Kernel has to handle the rest of the thing of the system for the Operating System. The Kernel remains in the memory until the Operating System is shut-down. Linux distribution (also called as a distro in short) is an operating system that is created from a collection of software built upon the Linux Kernel and is a package management system. A standard Linux distribution consists of a Linux kernel, GNU system, GNU utilities, libraries, compiler, additional software, documentation, a window system, window manager and a desktop environment. Most of the software included in a Linux distribution is free and open source. They may include some proprietary software like binary blobs which is essential for a few device drivers. Different Types of Shells \u00b6 Being able to interact with the kernel makes shells a powerful tool. Without the ability to interact with the kernel, a user cannot access the utilities offered by their machine\u2019s operating system. Let\u2019s take a look at some of the major shells that are available for the Linux environment Types of Shells Bourne Shell (sh) GNU Bourne-Again shell (bash) C Shell (csh) Korn Shell (ksh) Z Shell (zsh) Fish Shell (fish) Developed at AT&T Bell Labs by Steve Bourne , the Bourne shell is regarded as the first UNIX shell ever. It is denoted as sh. It gained popularity due to its compact nature and high speeds of operation. The GNU Bourne-Again shell was designed to be compatible with the Bourne shell. It incorporates useful features from different types of shells in Linux such as Korn shell and C shell. The shell's name bash is an acronym for \"Bourne Again Shell\", a pun on the name of the Bourne shell that it replaces and the notion of being \"born again\" First released in 1989, it has been used as the default login shell for most Linux distributions. Bash was also the default shell in all versions of Apple macOS prior to the 2019 release of macOS Catalina, which changed the default shell to zsh, although Bash remains available as an alternative shell The Bash command syntax is a superset of the Bourne shell command syntax. Bash supports brace expansion, command line completion (Programmable Completion),basic debugging and signal handling (using trap ) among other features. Bash can execute the vast majority of Bourne shell scripts without modification, with the exception of Bourne shell scripts stumbling into fringe syntax behavior interpreted differently in Bash or attempting to run a system command matching a newer Bash builtin, etc. The C shell was created at the University of California by Bill Joy. It is denoted as csh. It was developed to include useful programming features like in-built support for arithmetic operations and a syntax similar to the C programming language. Further, it incorporated command history which was missing in different types of shells in Linux like the Bourne shell. Another prominent feature of a C shell is \u201caliases\u201d. The complete path-name for the C shell is /bin/csh . By default, it uses the prompt hostname# for the root user and hostname% for the non-root users. The Korn shell was developed at AT&T Bell Labs by David Korn, to improve the Bourne shell. It is denoted as ksh. The Korn shell is essentially a superset of the Bourne shell. Besides supporting everything that would be supported by the Bourne shell, it provides users with new functionalities. It allows in-built support for arithmetic operations while offering interactive features which are similar to the C shell. The Korn shell runs scripts made for the Bourne shell, while offering string, array and function manipulation similar to the C programming language. It also supports scripts which were written for the C shell. Further, it is faster than most different types of shells zsh is a shell designed for interactive use, although it is also a powerful scripting language. Many of the useful features of bash, ksh, and tcsh were incorporated into zsh; many original features were added. Fish is a fully-equipped command line shell (like bash or zsh) that is smart and user-friendly. Fish supports powerful features like syntax highlighting, autosuggestions, and tab completions that just work, with nothing to learn or configure. If you want to make your command line more productive, more useful, and more fun, without learning a bunch of arcane syntax and configuration options, then fish might be just what you\u2019re looking for! UNIX Shell for Bioinformatics \u00b6 A shell is a computer program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard/touchscreen combination. There are many reasons to learn about the shell: Many bioinformatics tools can only be used through a command line interface. Many more have features and parameter options which are not available in the GUI. BLAST is an example. Many of the advanced functions are only accessible to users who know how to use a shell. The shell makes your work less boring. In bioinformatics you often need to repeat tasks with a large number of files. With the shell, you can automate those repetitive tasks and leave you free to do more exciting things. The shell makes your work less error-prone. When humans do the same thing a hundred different times (or even ten times), they\u2019re likely to make a mistake. Your computer can do the same thing a thousand times with no mistakes. The shell makes your work more reproducible. When you carry out your work in the command-line (rather than a GUI), your computer keeps a record of every step that you\u2019ve carried out, which you can use to re-do your work when you need to. It also gives you a way to communicate unambiguously what you\u2019ve done, so that others can inspect or apply your process to new data. Many bioinformatic tasks require large amounts of computing power and can\u2019t realistically be run on your own machine. These tasks are best performed using remote computers or cloud computing, which can only be accessed through a shell. Back to homepage","title":"What is a Unix Shell and Bash"},{"location":"1_introduction/#what-is-a-unix-shell-and-bash","text":"\u00ab Setup 3 - Shell Basics and recap \u00bb","title":"What is a Unix Shell and Bash"},{"location":"1_introduction/#the-unix-operating-system","text":"Unix is a multi-user operating system which allows more than one person to use the computer resources at a time. It was originally designed as a time-sharing system to serve several users simultaneous. Unix allows direct communication with the computer via a terminal, hence being very interactive and giving the user direct control over the computer resources. Unix also gives users the ability to share data and programs among one another. Unix is a generic operating system which takes full advantage of all available hardware such as 32-bit processor chips, expanded memory, and large, fast hard drives. Since Unix is written in a machine-independent language (C/C++) it is portable to many different types of machines including PC's. Therefore, Unix can be adapted to meet special requirements.The UNIX operating system is made up of three parts; the kernel, the shell and the programs. On Unix philosophy \u201cAlthough that philosophy can\u2019t be written down in a single sentence, as its heart is the idea that the power of a system comes more from the relationships among programs than from the programs themselves. Many UNIX programs do quite trivial things in isolation, but, combined with other programs, become general and useful tools.\u201d \u2013 Brian Kernighan & Rob Pike","title":"The UNIX operating system"},{"location":"1_introduction/#unix-vs-linux","text":"Linux is not Unix, but it is a \"Unix-like\" operating system. Linux system is derived from Unix and it is a continuation of the basis of Unix design. Linux distributions are the most famous and healthiest example of the direct Unix derivatives. BSD (Berkley Software Distribution) is also an example of a Unix derivative. Unix-like & a bit more on Linux A Unix-like OS (also called as UN*X or *nix) is the one that works in a way similar to Unix systems, however, it is not necessary that they conform to Single UNIX Specification (SUS) or similar POSIX (Portable Operating System Interface) standard. SUS is a standard which is required to be met for any OS to qualify for using \u2018UNIX\u2019 trademark. This trademark is granted by \u2018The Open Group\u2019 Few Examples of currently registered UNIX systems include macOS, Solaris, and AIX. If we consider the POSIX system, then Linux can be regarded as Unix-like OS. Linux is just the kernel and not the complete OS. This Linux kernel is generally packaged in Linux distributions which thereby makes it a complete OS. A Kernel is a computer program that is the heart and core of an Operating System. Since the Operating System has control over the system so, the Kernel also has control over everything in the system. It is the most important part of an Operating System. Whenever a system starts, the Kernel is the first program that is loaded after the bootloader because the Kernel has to handle the rest of the thing of the system for the Operating System. The Kernel remains in the memory until the Operating System is shut-down. Linux distribution (also called as a distro in short) is an operating system that is created from a collection of software built upon the Linux Kernel and is a package management system. A standard Linux distribution consists of a Linux kernel, GNU system, GNU utilities, libraries, compiler, additional software, documentation, a window system, window manager and a desktop environment. Most of the software included in a Linux distribution is free and open source. They may include some proprietary software like binary blobs which is essential for a few device drivers.","title":"UNIX vs. Linux"},{"location":"1_introduction/#different-types-of-shells","text":"Being able to interact with the kernel makes shells a powerful tool. Without the ability to interact with the kernel, a user cannot access the utilities offered by their machine\u2019s operating system. Let\u2019s take a look at some of the major shells that are available for the Linux environment Types of Shells Bourne Shell (sh) GNU Bourne-Again shell (bash) C Shell (csh) Korn Shell (ksh) Z Shell (zsh) Fish Shell (fish) Developed at AT&T Bell Labs by Steve Bourne , the Bourne shell is regarded as the first UNIX shell ever. It is denoted as sh. It gained popularity due to its compact nature and high speeds of operation. The GNU Bourne-Again shell was designed to be compatible with the Bourne shell. It incorporates useful features from different types of shells in Linux such as Korn shell and C shell. The shell's name bash is an acronym for \"Bourne Again Shell\", a pun on the name of the Bourne shell that it replaces and the notion of being \"born again\" First released in 1989, it has been used as the default login shell for most Linux distributions. Bash was also the default shell in all versions of Apple macOS prior to the 2019 release of macOS Catalina, which changed the default shell to zsh, although Bash remains available as an alternative shell The Bash command syntax is a superset of the Bourne shell command syntax. Bash supports brace expansion, command line completion (Programmable Completion),basic debugging and signal handling (using trap ) among other features. Bash can execute the vast majority of Bourne shell scripts without modification, with the exception of Bourne shell scripts stumbling into fringe syntax behavior interpreted differently in Bash or attempting to run a system command matching a newer Bash builtin, etc. The C shell was created at the University of California by Bill Joy. It is denoted as csh. It was developed to include useful programming features like in-built support for arithmetic operations and a syntax similar to the C programming language. Further, it incorporated command history which was missing in different types of shells in Linux like the Bourne shell. Another prominent feature of a C shell is \u201caliases\u201d. The complete path-name for the C shell is /bin/csh . By default, it uses the prompt hostname# for the root user and hostname% for the non-root users. The Korn shell was developed at AT&T Bell Labs by David Korn, to improve the Bourne shell. It is denoted as ksh. The Korn shell is essentially a superset of the Bourne shell. Besides supporting everything that would be supported by the Bourne shell, it provides users with new functionalities. It allows in-built support for arithmetic operations while offering interactive features which are similar to the C shell. The Korn shell runs scripts made for the Bourne shell, while offering string, array and function manipulation similar to the C programming language. It also supports scripts which were written for the C shell. Further, it is faster than most different types of shells zsh is a shell designed for interactive use, although it is also a powerful scripting language. Many of the useful features of bash, ksh, and tcsh were incorporated into zsh; many original features were added. Fish is a fully-equipped command line shell (like bash or zsh) that is smart and user-friendly. Fish supports powerful features like syntax highlighting, autosuggestions, and tab completions that just work, with nothing to learn or configure. If you want to make your command line more productive, more useful, and more fun, without learning a bunch of arcane syntax and configuration options, then fish might be just what you\u2019re looking for!","title":"Different Types of Shells"},{"location":"1_introduction/#unix-shell-for-bioinformatics","text":"A shell is a computer program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard/touchscreen combination. There are many reasons to learn about the shell: Many bioinformatics tools can only be used through a command line interface. Many more have features and parameter options which are not available in the GUI. BLAST is an example. Many of the advanced functions are only accessible to users who know how to use a shell. The shell makes your work less boring. In bioinformatics you often need to repeat tasks with a large number of files. With the shell, you can automate those repetitive tasks and leave you free to do more exciting things. The shell makes your work less error-prone. When humans do the same thing a hundred different times (or even ten times), they\u2019re likely to make a mistake. Your computer can do the same thing a thousand times with no mistakes. The shell makes your work more reproducible. When you carry out your work in the command-line (rather than a GUI), your computer keeps a record of every step that you\u2019ve carried out, which you can use to re-do your work when you need to. It also gives you a way to communicate unambiguously what you\u2019ve done, so that others can inspect or apply your process to new data. Many bioinformatic tasks require large amounts of computing power and can\u2019t realistically be run on your own machine. These tasks are best performed using remote computers or cloud computing, which can only be accessed through a shell. Back to homepage","title":"UNIX Shell for Bioinformatics"},{"location":"2_unixshellbasics/","text":"Unix Shell Basics \u00b6 \u00ab 1 - Unix Shell and Bash 4 - Text Manipu. Pt1 \u00bb It is expected that you are already familiar with using the basics of the Unix Shell. As a quick refresher, some frequently used commands are listed below. For more information about a command, use the Unix man command. For example, to get more information about the mkdir command, type: $ man mkdir Key commands for navigating around the filesystem are: ls - list the contents of the current directory ls -l - list the contents of the current directory in more detail. pwd - show the location of the current directory cd DIR - change directory to directory DIR (DIR must be in your current directory - you should see its name when you type ls OR you need to specify either a full or relative path to DIR) cd - - change back to the last directory you were in. cd (also cd ~/ )- change to your home directory cd .. - change to the directory one level above Other useful commands: mv - move files or directories cp - copy files or directories rm - delete files or directories mkdir - create a new directory cat - concatenate and print text files to screen more - show contents of text files on screen less - cooler version of more . Allows searching (use / ) tree - tree view of directory structure head - view lines from the start of a file tail - view lines from the end of a file grep - find patterns within files Output redirection \u00b6 The data_shell4b directory contains the following fasta files: tb1.fasta tb1-protein.fasta tga1-protein.fasta We can use the cat command to view these files either one at a time: $ cat tb1-protein.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA $ cat tga1-protein.fasta >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT OR all at once with cat *.fasta We can also redirect the output to create a new file containing the sequence for both proteins: $ cat tb1-protein.fasta tga1-protein.fasta > zea-proteins.fasta Now we have a new file called zea-proteins.fasta . Let's check the contents: $ cat zea-proteins.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT Capturing error messages $ cat tb1-protein.fasta mik.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA cat: mik.fasta: No such file or directory There are two different types of output there: \"standard outout\" (the contents of the tb1-protein.fasta file) and standard error (the error message relatign to the missign mik.fasta file). If we use the > operator to redirect the outout, the standard output is catured, bu the standard error is not - it is still printed to the screen. Let's check: $ cat tb1-protein.fasta mik.fasta > test.fasta cat: mik.fasta: No such file or directory The new file has been created, and contans the standard outout (contents of the file tb1-protein.fasta ): $ cat test.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA If we want to capture the standard error, we use the (slightly unweildy) 2> operator: $ cat tb1-protein.fasta mik.fasta > test.fasta 2 > stderror.txt Descriptors File descriptor 2 represents standard error. (other special file descriptors include 0 for standard input and 1 for standard output). Check the contents: $ cat stderror.txt cat: mik.fasta: No such file or directory Note that > will overwrite an existing file. We can use >> to add to a file instead of overwriting it: $ cat tga1-protein.fasta >> test.fasta $ cat test.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT The Unix pipe \u00b6 The pipe operator ( | ) passes the output from one command to another command as input. The following is an example of using a pipe with the grep command. Steps: Remove the header information for the sequence (line starts with \">\") Highlight any characters in the sequence that are not A, T, C or G. We will use grep to carry out the first step, and then use the pipe operator to pass the output to a second grep command to carry out the second step. Here is the full command: $ grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" Let's see what each piece does grep -v \"^>\" tb1.fasta This tells grep to search for all lines in the file tb1.fasta that do not contain a \">\" at the start ( ^ is a special character that denotes \"at the start of the line - we'll learn more about this later). grep --color -i \"[^ATCG]\" There are a few things going on here: --color : tells grep to highlight any matches -i : tells grep to ignore the case (i.e., will match lower or upper case) [^ATCG] : when ^ is used inside square brackets it has a different function - inverts the pattern, so that grep finds any letters that are not A, T, C or G. Let's run the code: $ grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" CCCCAAAGACGGACCAATCCAGCAGCTTCTACTGCTAYCCATGCTCCCCTCCCTTCGCCGCCGCCGACGC NB: colours not supported in Markdown (I tried using HTML) What if we had just run the code for step 2 on the tb1.fasta file? $ grep --color -i \"[^ATCG]\" tb1.fasta Combining pipes and redirection \u00b6 $ grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" > non-atcg.txt $ cat non-atcg.txt since we are redirecting to a text file, the --color by itself will not record the colour information. We can achieve this by invoking always flag for --color .i.e.. $ grep -v \"^>\" tb1.fasta | grep --color = always -i \"[^ATCG]\" > non-atcg.txt Using tee to capture intermediate outputs \u00b6 $ grep -v \"^>\" tb1.fasta | tee intermediate-file.txt | grep --color = always -i \"[^ATCG]\" > non-atcg.txt The file intermediate-file.txt will contain the output from grep -v \"^>\" tb1.fasta , but tee also passes that output through the pipe to the next grep command. Back to homepage","title":"Unix Shell Basics"},{"location":"2_unixshellbasics/#unix-shell-basics","text":"\u00ab 1 - Unix Shell and Bash 4 - Text Manipu. Pt1 \u00bb It is expected that you are already familiar with using the basics of the Unix Shell. As a quick refresher, some frequently used commands are listed below. For more information about a command, use the Unix man command. For example, to get more information about the mkdir command, type: $ man mkdir Key commands for navigating around the filesystem are: ls - list the contents of the current directory ls -l - list the contents of the current directory in more detail. pwd - show the location of the current directory cd DIR - change directory to directory DIR (DIR must be in your current directory - you should see its name when you type ls OR you need to specify either a full or relative path to DIR) cd - - change back to the last directory you were in. cd (also cd ~/ )- change to your home directory cd .. - change to the directory one level above Other useful commands: mv - move files or directories cp - copy files or directories rm - delete files or directories mkdir - create a new directory cat - concatenate and print text files to screen more - show contents of text files on screen less - cooler version of more . Allows searching (use / ) tree - tree view of directory structure head - view lines from the start of a file tail - view lines from the end of a file grep - find patterns within files","title":"Unix Shell Basics"},{"location":"2_unixshellbasics/#output-redirection","text":"The data_shell4b directory contains the following fasta files: tb1.fasta tb1-protein.fasta tga1-protein.fasta We can use the cat command to view these files either one at a time: $ cat tb1-protein.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA $ cat tga1-protein.fasta >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT OR all at once with cat *.fasta We can also redirect the output to create a new file containing the sequence for both proteins: $ cat tb1-protein.fasta tga1-protein.fasta > zea-proteins.fasta Now we have a new file called zea-proteins.fasta . Let's check the contents: $ cat zea-proteins.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT Capturing error messages $ cat tb1-protein.fasta mik.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA cat: mik.fasta: No such file or directory There are two different types of output there: \"standard outout\" (the contents of the tb1-protein.fasta file) and standard error (the error message relatign to the missign mik.fasta file). If we use the > operator to redirect the outout, the standard output is catured, bu the standard error is not - it is still printed to the screen. Let's check: $ cat tb1-protein.fasta mik.fasta > test.fasta cat: mik.fasta: No such file or directory The new file has been created, and contans the standard outout (contents of the file tb1-protein.fasta ): $ cat test.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA If we want to capture the standard error, we use the (slightly unweildy) 2> operator: $ cat tb1-protein.fasta mik.fasta > test.fasta 2 > stderror.txt Descriptors File descriptor 2 represents standard error. (other special file descriptors include 0 for standard input and 1 for standard output). Check the contents: $ cat stderror.txt cat: mik.fasta: No such file or directory Note that > will overwrite an existing file. We can use >> to add to a file instead of overwriting it: $ cat tga1-protein.fasta >> test.fasta $ cat test.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT","title":"Output redirection"},{"location":"2_unixshellbasics/#the-unix-pipe","text":"The pipe operator ( | ) passes the output from one command to another command as input. The following is an example of using a pipe with the grep command. Steps: Remove the header information for the sequence (line starts with \">\") Highlight any characters in the sequence that are not A, T, C or G. We will use grep to carry out the first step, and then use the pipe operator to pass the output to a second grep command to carry out the second step. Here is the full command: $ grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" Let's see what each piece does grep -v \"^>\" tb1.fasta This tells grep to search for all lines in the file tb1.fasta that do not contain a \">\" at the start ( ^ is a special character that denotes \"at the start of the line - we'll learn more about this later). grep --color -i \"[^ATCG]\" There are a few things going on here: --color : tells grep to highlight any matches -i : tells grep to ignore the case (i.e., will match lower or upper case) [^ATCG] : when ^ is used inside square brackets it has a different function - inverts the pattern, so that grep finds any letters that are not A, T, C or G. Let's run the code: $ grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" CCCCAAAGACGGACCAATCCAGCAGCTTCTACTGCTAYCCATGCTCCCCTCCCTTCGCCGCCGCCGACGC NB: colours not supported in Markdown (I tried using HTML) What if we had just run the code for step 2 on the tb1.fasta file? $ grep --color -i \"[^ATCG]\" tb1.fasta","title":"The Unix pipe"},{"location":"2_unixshellbasics/#combining-pipes-and-redirection","text":"$ grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" > non-atcg.txt $ cat non-atcg.txt since we are redirecting to a text file, the --color by itself will not record the colour information. We can achieve this by invoking always flag for --color .i.e.. $ grep -v \"^>\" tb1.fasta | grep --color = always -i \"[^ATCG]\" > non-atcg.txt","title":"Combining pipes and redirection"},{"location":"2_unixshellbasics/#using-tee-to-capture-intermediate-outputs","text":"$ grep -v \"^>\" tb1.fasta | tee intermediate-file.txt | grep --color = always -i \"[^ATCG]\" > non-atcg.txt The file intermediate-file.txt will contain the output from grep -v \"^>\" tb1.fasta , but tee also passes that output through the pipe to the next grep command. Back to homepage","title":"Using tee to capture intermediate outputs"},{"location":"4_inspectmanipluate/","text":"Inspecting and Manipulating Text Data with Unix Tools - Part 1 \u00b6 \u00ab 3 - Shell Basics and recap 5 - Text Manipu. Pt2 \u00bb Many formats in bioinformatics are simple tabular plain-text files delimited by a character. The most common tabular plain-text file format used in bioinformatics is tab-delimited. Bioinformatics evolved to favor tab-delimited formats because of the convenience of working with these files using Unix tools. Tabular Plain-Text Data Formats Tabular plain-text data formats are used extensively in computing. The basic format is incredibly simple: each row (also known as a record) is kept on its own line, and each column (also known as a field) is separated by some delimiter. There are three flavors you will encounter: tab-delimited, comma-separated, and variable space-delimited. Inspecting data with head and tail \u00b6 Although cat command is an easy way for us to open and view the content of a file, it is not very practical to do so for a file with thousands of lines as it will exhaust the shell \"space\". Instead, large files should be inspected first and then manipulated accordingly. First round of inspection can be done with head and tail command which prints the first 10 lines and the last 10 lines ( -n 10 ) of a a file, respectively. .i.e. Let's use head and tail to inspect Mus_musculus.GRCm38.75_chr1.bed $ head Mus_musculus.GRCm38.75_chr1.bed 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 1 3102016 3102125 1 3102016 3102125 1 3102016 3102125 1 3205901 3671498 1 3205901 3216344 1 3213609 3216344 1 3205901 3207317 $ tail Mus_musculus.GRCm38.75_chr1.bed 1 195166217 195166390 1 195165745 195165851 1 195165748 195165851 1 195165745 195165747 1 195228278 195228398 1 195228278 195228398 1 195228278 195228398 1 195240910 195241007 1 195240910 195241007 1 195240910 195241007 Changing the number of lines printed for either of those commands can be done by passing -n <number_of_lines> flag .i.e. Over-ride the -n 10 default Try those commands with 0n 4 to print top 4 lines and bottom 4 lines $ head -n 4 Mus_musculus.GRCm38.75_chr1.bed $ tail -n 4 Mus_musculus.GRCm38.75_chr1.bed Exercise 4.1 Sometimes it\u2019s useful to see both the beginning and end of a file \u2014 for example, if we have a sorted BED file and we want to see the positions of the first feature and last feature. Can you figure out a way to use both head and tail in a single command to inspect first and last 2 lines of Mus_musculus.GRCm38.75_chr1.bed ? Exercise 4.2 We can also use tail to remove the header of a file. Normally the -n argument specifies how many of the last lines of a file to include, but if -n is given a number x preceded with a + sign (e.g., +x ), tail will start from the x th line. So to chop off a header, we start from the second line with -n +2 . Use the seq command to generate a file containing the numbers 1 to 10, and then use the tail command to chop off the first line. Extract summary information with wc \u00b6 The \"wc\" in the wc command which stands for \"word count\" - this command can count the numbers of words, lines , and characters in a file (take a note on the order). $ wc Mus_musculus.GRCm38.75_chr1.bed 81226 243678 1698545 Mus_musculus.GRCm38.75_chr1.bed Often, we only need to list the number of lines, which can be done by using the -l flag. It can be used as a sanity check - for example, to make sure an output has the same number of lines as the input, OR to check that a certain file format which depends on another format without losing overall data structure wasn't corrupted or over/under manipulated. Qu. Count the number of lines in Mus_musculus.GRCm38.75_chr1.bed and Mus_musculus.GRCm38.75_chr1.gtf . Anything out of the ordinary ? Although wc -l is the quickest way to count the number of lines in a file, it is not the most robust as it relies on the very bad assumption that \"data is well formatted\" For an example, If we are to create a file with 3 rows of data and then two empty lines, $ cat > fool_wc.bed 1 100 2 200 3 300 $ $ wc -l fool_wc.bed 5 fool_wc.bed This is a good place bring in grep again which can be used to count the number of lines while excluding white-spaces (spaces, tabs or newlines) $ grep -c \"[^ \\\\n\\\\t]\" fool_wc.bed 3 Using cut with column data and formatting tabular data with column \u00b6 When working with plain-text tabular data formats like tab-delimited and CSV files, we often need to extract specific columns from the original file or stream. For example, suppose we wanted to extract only the start positions (the second column) of the Mus_musculus.GRCm38.75_chr1.bed file. The simplest way to do this is with cut . $ cut -f 2 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054233 3054233 -f argument is how we specify which columns to keep. It can be used to specify a range as well $ cut -f 2 -3 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054733 3054233 3054733 3054233 3054733 ``` Using ` cut ` , we can convert our GTF for ***Mus_musculus.GRCm38.75_chr1.gtf*** to a three-column tab-delimited file of genomic ranges ( e.g., chromosome, start, and end position ) . We\u2019ll chop off the metadata rows using the grep command covered earlier, and then use cut to extract the first, fourth, and fifth columns ( chromosome, start, end ) : ``` bash $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f1,4,5 | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 Note that although our three-column file of genomic positions looks like a BED- formatted file, it\u2019s not due to subtle differences in genomic range formats cut also allows us to specify the column delimiter character. So, if we were to come across a CSV file containing chromosome names, start positions, and end positions, we could select columns from it, too: As you may have noticed when working with tab-delimited files, it\u2019s not always easy to see which elements belong to a particular column. For example: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f1-8 | head -n3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . While tabs are a great delimiter in plain-text data files, our variable width data leads our columns to not stack up well. There\u2019s a fix for this in Unix: program column -t (the -t option tells column to treat data as a table). column -t produces neat columns that are much easier to read: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1 -8 | column -t | head -n 3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . Note that you should only use column -t to visualize data in the terminal, not to reformat data to write to a file. Tab-delimited data is preferable to data delimited by a variable number of spaces, since it\u2019s easier for programs to parse. Like cut , column \u2019s default delimiter is the tab character ( \\t ). We can specify a different delimiter with the -s option. So, if we wanted to visualize the columns of the Mus_musculus.GRCm38.75_chr1_bed.csv file more easily, we could use: $ column -s \",\" -t Mus_musculus.GRCm38.75_chr1.bed | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 Sorting Plain-Text Data with sort \u00b6 Very often we need to work with sorted plain-text data in bioinformatics. The two most common reasons to sort data are as follows: - Certain operations are much more efficient when performed on sorted data. - Sorting data is a prerequisite to finding all unique lines sort is designed to work with plain-text data with columns. Create a test .bed file with few rows and use sort command without any arguments $ cat > test_sort.bed chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 $ sort test_sort.bed chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr2 35 54 chr3 11 28 chr3 16 27 sort without any arguments simply sorts a file alphanumerically by line. Because chromosome is the first column, sorting by line effectively groups chromosomes together, as these are \"ties\" in the sorted order. However, using sort \u2019s defaults of sorting alphanumerically by line doesn\u2019t handle tabular data properly. There are two new features we need: The ability to sort by particular columns The ability to tell sort that certain columns are numeric values (and not alpha\u2010numeric text) sort has a simple syntax to do this. Let\u2019s look at how we\u2019d sort example.bed by chromosome (first column), and start position (second column): $ sort -k1,1 -k2n test_sort.bed chr1 9 28 chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr2 35 54 chr3 11 28 chr3 16 27 Here, we specify the columns (and their order) we want to sort by as -k arguments. In technical terms, -k specifies the sorting keys and their order. Each -k argument takes a range of columns as start,end , so to sort by a single column we use start,start . In the preceding example, we first sorted by the first column (chromosome), as the first -k argument was -k1,1 . Sorting by the first column alone leads to many ties in rows with the same chromosomes (e.g., \u201cchr1\u201d and \u201cchr3\u201d). Adding a second -k argument with a different column tells sort how to break these ties. In our example, -k2,2n tells sort to sort by the second column (start position), treating this column as numerical data (because there\u2019s an n in -k2,2n ). Exercise 4.3 Mus_musculus.GRCm38.75_chr1_random.gtf file is Mus_musculus.GRCm38.75_chr1.gtf with permuted rows (and without a metadata header). Can you group rows by chromosome, and sort by position? If yes, append the output to a separate file. Finding Unique Values with uniq \u00b6 uniq takes lines from a file or standard input stream, and outputs all lines with consecutive duplicates removed. While this is a relatively simple functionality, you will use uniq very frequently in command-line data processing. $ cat letters.txt A A B C B C C C $ uniq letters.txt A B C B C As you can see, uniq does not return the unique values in letters.txt \u2014 it only removes consecutive duplicate lines (keeping one). If instead we did want to find all unique lines in a file, we would first sort all lines using sort so that all identical lines are grouped next to each other, and then run uniq . $ sort letters.txt | uniq A B C uniq with -c shows the counts of occurrences next to the unique lines. $ uniq -c letters.txt 2 A 1 B 1 C 1 B 3 C $ sort letters.txt | uniq -c 2 A 2 B 4 C Combined with other Unix tools like cut , grep and sort , uniq can be used to summarize columns of tabular data: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c 25901 CDS 36128 exon 2027 gene 2290 start_codon 2299 stop_codon 4993 transcript 7588 UTR Count in order from most frequent to last $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c | sort -rn 36128 exon 25901 CDS 7588 UTR 4993 transcript 2299 stop_codon 2290 start_codon 2027 gene sed \u00b6 The s treamline ed itor or sed command is a stream editor that reads one or more text files, makes changes or edits according to editing script, and writes the results to standard output. First, we will discuss sed command with respect to search and replace function. Find and Replace \u00b6 Most common use of sed is to substitute text, matching a pattern. This syntax for doing this in sed is as follows: sed 'OPERATION/REGEXP/REPLACEMENT/FLAGS' FILENAME Here, / is the delimiter (you can also use _ (underscore), | (pipe) or : (colon) as delimiter as well) OPERATION specifies the action to be performed (sometimes if a condition is satisfied). The most common and widely used operation is s which does the substitution operation (other useful operators include y for transformation, i for insertion, d for deletion etc.). REGEXP and REPLACEMENT specify search term and the substitution term respectively for the operation that is being performed. FLAGS are additional parameters that control the operation. Some common FLAGS include: g replace all the instances of REGEXP with REPLACEMENT (globally) N where N is any number, to replace Nth instance of the REGEXP with REPLACEMENT p if substitution was made, then prints the new pattern space i ignores case for matching REGEXP w file If substitution was made, write out the result to the given file d when specified without REPLACEMENT , deletes the found REGEXP Some find and replace examples Find and replace all chr to chromosome in the example.bed file and append the the edit to a new file names example_chromosome.bed $ sed 's/chr/chromosome/g' example.bed > example_chromosome.bed Find and replace chr to chromosome , only if you also find 40 in the line $ sed '/40/s/chr/chromosome/g' example.bed > example_40.bed Find and replace directly on the input, but save a old version too $ sed -i.old 's/chr/chromosome/g' example.bed Print specific lines of the file To print a specific line, you can use the address function, note that by default, sed will stream the entire file, so when you are interested in specific lines only, you will have to suppress this feature using the option -n . print 5 th line of example.bed $ sed -n '5p' example.bed We can provide any number of additional lines to print using -e option. Let's print line 2 and 5, $ sed -n -e '2p' -e '5p' example.bed It also accepts range, using , . Let's print line 2-6, $ sed -n '2,6p' example.bed Also, we can create specific pattern, like multiple of a number using ~ . Let's print every tenth line of Mus_musculus.GRCm38.75_chr1.bed starting from 10, 20, 30.. to end of the file $ sed -n '10~10p' Mus_musculus.GRCm38.75_chr1.bed Exercise 4.4 Can you use the above ~ trick to extract all the odd numbered lines from Mus_musculus.GRCm38.75_chr1.bed and append the output to a new file odd_sed.bed One of the powerful feature is that we can combine these ranges or multiples in any fashion. Example: fastq files have header on first line and sequence in second, next two lines will have the quality and a blank extra line (four lines make one read). Sometimes we only need the sequence and header $ sed -n '1~4p;2~4p' SRR097977.fastq Sanity Check It's not a bad practice validate some of these commands by comparing the output from another command. For an example, above sed -n '1~4p;2~4p' SRR097977.fastq should print exactly half the number of lines in the file as it is removing two lines per read. Do a quick sanity check with $ sed -n '1~4p;2~4p' SRR097977.fastq | wc -l & $ cat SRR097977.fastq | wc -l We can use the above trick to convert the .fastq to .fasta $ sed -n '1~4p;2~4p' SRR097977.fastq | sed 's/^@/>/g' > SRR097977.fasta Let's wrap up sed with one more use case (slightly complicated looking one). Let's say that we want capture all the transcript names from the last column (9 th column) from .gtf file. We can write something similar to , $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 | sed -E 's/.*transcript_id \"([^\"]+)\".*/\\1/' Not really what we are after as the output is as below 1 pseudogene gene 3054233 3054733 . + . gene_id \"ENSMUSG00000090025\" ; gene_name \"Gm16088\" ; gene_source \"havana\" ; gene_biotype \"pseudogene\" ; ENSMUST00000160944 ENSMUST00000160944 The is due to sed default behaviour where it prints every line, making replacements to matching lines. .i.e Some lines of the last column of Mus_musculus.GRCm38.75_chr1.gtf don't contain transcript_id . So, sed prints the entire line rather than captured group. One way to solve this would be to use grep transcript_id before sed to only work with lines containing the string transcript_id . However, sed offers a cleaner way. First, disable sed from outputting all lines with -n . Then, by appending p after the last slash sed will print all lines it\u2019s made a replacement on. The following is an illustration of -n used with p : $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 | sed -E -n 's/.*transcript_id \"([^\"]+)\".*/\\1/p' Back to homepage","title":"Inspecting and Manipulating Text Data with Unix Tools - Part 1"},{"location":"4_inspectmanipluate/#inspecting-and-manipulating-text-data-with-unix-tools-part-1","text":"\u00ab 3 - Shell Basics and recap 5 - Text Manipu. Pt2 \u00bb Many formats in bioinformatics are simple tabular plain-text files delimited by a character. The most common tabular plain-text file format used in bioinformatics is tab-delimited. Bioinformatics evolved to favor tab-delimited formats because of the convenience of working with these files using Unix tools. Tabular Plain-Text Data Formats Tabular plain-text data formats are used extensively in computing. The basic format is incredibly simple: each row (also known as a record) is kept on its own line, and each column (also known as a field) is separated by some delimiter. There are three flavors you will encounter: tab-delimited, comma-separated, and variable space-delimited.","title":"Inspecting and Manipulating Text Data with Unix Tools - Part 1"},{"location":"4_inspectmanipluate/#inspecting-data-with-head-and-tail","text":"Although cat command is an easy way for us to open and view the content of a file, it is not very practical to do so for a file with thousands of lines as it will exhaust the shell \"space\". Instead, large files should be inspected first and then manipulated accordingly. First round of inspection can be done with head and tail command which prints the first 10 lines and the last 10 lines ( -n 10 ) of a a file, respectively. .i.e. Let's use head and tail to inspect Mus_musculus.GRCm38.75_chr1.bed $ head Mus_musculus.GRCm38.75_chr1.bed 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 1 3102016 3102125 1 3102016 3102125 1 3102016 3102125 1 3205901 3671498 1 3205901 3216344 1 3213609 3216344 1 3205901 3207317 $ tail Mus_musculus.GRCm38.75_chr1.bed 1 195166217 195166390 1 195165745 195165851 1 195165748 195165851 1 195165745 195165747 1 195228278 195228398 1 195228278 195228398 1 195228278 195228398 1 195240910 195241007 1 195240910 195241007 1 195240910 195241007 Changing the number of lines printed for either of those commands can be done by passing -n <number_of_lines> flag .i.e. Over-ride the -n 10 default Try those commands with 0n 4 to print top 4 lines and bottom 4 lines $ head -n 4 Mus_musculus.GRCm38.75_chr1.bed $ tail -n 4 Mus_musculus.GRCm38.75_chr1.bed Exercise 4.1 Sometimes it\u2019s useful to see both the beginning and end of a file \u2014 for example, if we have a sorted BED file and we want to see the positions of the first feature and last feature. Can you figure out a way to use both head and tail in a single command to inspect first and last 2 lines of Mus_musculus.GRCm38.75_chr1.bed ? Exercise 4.2 We can also use tail to remove the header of a file. Normally the -n argument specifies how many of the last lines of a file to include, but if -n is given a number x preceded with a + sign (e.g., +x ), tail will start from the x th line. So to chop off a header, we start from the second line with -n +2 . Use the seq command to generate a file containing the numbers 1 to 10, and then use the tail command to chop off the first line.","title":"Inspecting data with head and tail"},{"location":"4_inspectmanipluate/#extract-summary-information-with-wc","text":"The \"wc\" in the wc command which stands for \"word count\" - this command can count the numbers of words, lines , and characters in a file (take a note on the order). $ wc Mus_musculus.GRCm38.75_chr1.bed 81226 243678 1698545 Mus_musculus.GRCm38.75_chr1.bed Often, we only need to list the number of lines, which can be done by using the -l flag. It can be used as a sanity check - for example, to make sure an output has the same number of lines as the input, OR to check that a certain file format which depends on another format without losing overall data structure wasn't corrupted or over/under manipulated. Qu. Count the number of lines in Mus_musculus.GRCm38.75_chr1.bed and Mus_musculus.GRCm38.75_chr1.gtf . Anything out of the ordinary ? Although wc -l is the quickest way to count the number of lines in a file, it is not the most robust as it relies on the very bad assumption that \"data is well formatted\" For an example, If we are to create a file with 3 rows of data and then two empty lines, $ cat > fool_wc.bed 1 100 2 200 3 300 $ $ wc -l fool_wc.bed 5 fool_wc.bed This is a good place bring in grep again which can be used to count the number of lines while excluding white-spaces (spaces, tabs or newlines) $ grep -c \"[^ \\\\n\\\\t]\" fool_wc.bed 3","title":"Extract summary information with wc"},{"location":"4_inspectmanipluate/#using-cut-with-column-data-and-formatting-tabular-data-with-column","text":"When working with plain-text tabular data formats like tab-delimited and CSV files, we often need to extract specific columns from the original file or stream. For example, suppose we wanted to extract only the start positions (the second column) of the Mus_musculus.GRCm38.75_chr1.bed file. The simplest way to do this is with cut . $ cut -f 2 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054233 3054233 -f argument is how we specify which columns to keep. It can be used to specify a range as well $ cut -f 2 -3 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054733 3054233 3054733 3054233 3054733 ``` Using ` cut ` , we can convert our GTF for ***Mus_musculus.GRCm38.75_chr1.gtf*** to a three-column tab-delimited file of genomic ranges ( e.g., chromosome, start, and end position ) . We\u2019ll chop off the metadata rows using the grep command covered earlier, and then use cut to extract the first, fourth, and fifth columns ( chromosome, start, end ) : ``` bash $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f1,4,5 | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 Note that although our three-column file of genomic positions looks like a BED- formatted file, it\u2019s not due to subtle differences in genomic range formats cut also allows us to specify the column delimiter character. So, if we were to come across a CSV file containing chromosome names, start positions, and end positions, we could select columns from it, too: As you may have noticed when working with tab-delimited files, it\u2019s not always easy to see which elements belong to a particular column. For example: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f1-8 | head -n3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . While tabs are a great delimiter in plain-text data files, our variable width data leads our columns to not stack up well. There\u2019s a fix for this in Unix: program column -t (the -t option tells column to treat data as a table). column -t produces neat columns that are much easier to read: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1 -8 | column -t | head -n 3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . Note that you should only use column -t to visualize data in the terminal, not to reformat data to write to a file. Tab-delimited data is preferable to data delimited by a variable number of spaces, since it\u2019s easier for programs to parse. Like cut , column \u2019s default delimiter is the tab character ( \\t ). We can specify a different delimiter with the -s option. So, if we wanted to visualize the columns of the Mus_musculus.GRCm38.75_chr1_bed.csv file more easily, we could use: $ column -s \",\" -t Mus_musculus.GRCm38.75_chr1.bed | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733","title":"Using cut with column data and formatting tabular data with column"},{"location":"4_inspectmanipluate/#sorting-plain-text-data-with-sort","text":"Very often we need to work with sorted plain-text data in bioinformatics. The two most common reasons to sort data are as follows: - Certain operations are much more efficient when performed on sorted data. - Sorting data is a prerequisite to finding all unique lines sort is designed to work with plain-text data with columns. Create a test .bed file with few rows and use sort command without any arguments $ cat > test_sort.bed chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 $ sort test_sort.bed chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr2 35 54 chr3 11 28 chr3 16 27 sort without any arguments simply sorts a file alphanumerically by line. Because chromosome is the first column, sorting by line effectively groups chromosomes together, as these are \"ties\" in the sorted order. However, using sort \u2019s defaults of sorting alphanumerically by line doesn\u2019t handle tabular data properly. There are two new features we need: The ability to sort by particular columns The ability to tell sort that certain columns are numeric values (and not alpha\u2010numeric text) sort has a simple syntax to do this. Let\u2019s look at how we\u2019d sort example.bed by chromosome (first column), and start position (second column): $ sort -k1,1 -k2n test_sort.bed chr1 9 28 chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr2 35 54 chr3 11 28 chr3 16 27 Here, we specify the columns (and their order) we want to sort by as -k arguments. In technical terms, -k specifies the sorting keys and their order. Each -k argument takes a range of columns as start,end , so to sort by a single column we use start,start . In the preceding example, we first sorted by the first column (chromosome), as the first -k argument was -k1,1 . Sorting by the first column alone leads to many ties in rows with the same chromosomes (e.g., \u201cchr1\u201d and \u201cchr3\u201d). Adding a second -k argument with a different column tells sort how to break these ties. In our example, -k2,2n tells sort to sort by the second column (start position), treating this column as numerical data (because there\u2019s an n in -k2,2n ). Exercise 4.3 Mus_musculus.GRCm38.75_chr1_random.gtf file is Mus_musculus.GRCm38.75_chr1.gtf with permuted rows (and without a metadata header). Can you group rows by chromosome, and sort by position? If yes, append the output to a separate file.","title":"Sorting Plain-Text Data with sort"},{"location":"4_inspectmanipluate/#finding-unique-values-with-uniq","text":"uniq takes lines from a file or standard input stream, and outputs all lines with consecutive duplicates removed. While this is a relatively simple functionality, you will use uniq very frequently in command-line data processing. $ cat letters.txt A A B C B C C C $ uniq letters.txt A B C B C As you can see, uniq does not return the unique values in letters.txt \u2014 it only removes consecutive duplicate lines (keeping one). If instead we did want to find all unique lines in a file, we would first sort all lines using sort so that all identical lines are grouped next to each other, and then run uniq . $ sort letters.txt | uniq A B C uniq with -c shows the counts of occurrences next to the unique lines. $ uniq -c letters.txt 2 A 1 B 1 C 1 B 3 C $ sort letters.txt | uniq -c 2 A 2 B 4 C Combined with other Unix tools like cut , grep and sort , uniq can be used to summarize columns of tabular data: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c 25901 CDS 36128 exon 2027 gene 2290 start_codon 2299 stop_codon 4993 transcript 7588 UTR Count in order from most frequent to last $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c | sort -rn 36128 exon 25901 CDS 7588 UTR 4993 transcript 2299 stop_codon 2290 start_codon 2027 gene","title":"Finding Unique Values with uniq"},{"location":"4_inspectmanipluate/#sed","text":"The s treamline ed itor or sed command is a stream editor that reads one or more text files, makes changes or edits according to editing script, and writes the results to standard output. First, we will discuss sed command with respect to search and replace function.","title":"sed"},{"location":"4_inspectmanipluate/#find-and-replace","text":"Most common use of sed is to substitute text, matching a pattern. This syntax for doing this in sed is as follows: sed 'OPERATION/REGEXP/REPLACEMENT/FLAGS' FILENAME Here, / is the delimiter (you can also use _ (underscore), | (pipe) or : (colon) as delimiter as well) OPERATION specifies the action to be performed (sometimes if a condition is satisfied). The most common and widely used operation is s which does the substitution operation (other useful operators include y for transformation, i for insertion, d for deletion etc.). REGEXP and REPLACEMENT specify search term and the substitution term respectively for the operation that is being performed. FLAGS are additional parameters that control the operation. Some common FLAGS include: g replace all the instances of REGEXP with REPLACEMENT (globally) N where N is any number, to replace Nth instance of the REGEXP with REPLACEMENT p if substitution was made, then prints the new pattern space i ignores case for matching REGEXP w file If substitution was made, write out the result to the given file d when specified without REPLACEMENT , deletes the found REGEXP Some find and replace examples Find and replace all chr to chromosome in the example.bed file and append the the edit to a new file names example_chromosome.bed $ sed 's/chr/chromosome/g' example.bed > example_chromosome.bed Find and replace chr to chromosome , only if you also find 40 in the line $ sed '/40/s/chr/chromosome/g' example.bed > example_40.bed Find and replace directly on the input, but save a old version too $ sed -i.old 's/chr/chromosome/g' example.bed Print specific lines of the file To print a specific line, you can use the address function, note that by default, sed will stream the entire file, so when you are interested in specific lines only, you will have to suppress this feature using the option -n . print 5 th line of example.bed $ sed -n '5p' example.bed We can provide any number of additional lines to print using -e option. Let's print line 2 and 5, $ sed -n -e '2p' -e '5p' example.bed It also accepts range, using , . Let's print line 2-6, $ sed -n '2,6p' example.bed Also, we can create specific pattern, like multiple of a number using ~ . Let's print every tenth line of Mus_musculus.GRCm38.75_chr1.bed starting from 10, 20, 30.. to end of the file $ sed -n '10~10p' Mus_musculus.GRCm38.75_chr1.bed Exercise 4.4 Can you use the above ~ trick to extract all the odd numbered lines from Mus_musculus.GRCm38.75_chr1.bed and append the output to a new file odd_sed.bed One of the powerful feature is that we can combine these ranges or multiples in any fashion. Example: fastq files have header on first line and sequence in second, next two lines will have the quality and a blank extra line (four lines make one read). Sometimes we only need the sequence and header $ sed -n '1~4p;2~4p' SRR097977.fastq Sanity Check It's not a bad practice validate some of these commands by comparing the output from another command. For an example, above sed -n '1~4p;2~4p' SRR097977.fastq should print exactly half the number of lines in the file as it is removing two lines per read. Do a quick sanity check with $ sed -n '1~4p;2~4p' SRR097977.fastq | wc -l & $ cat SRR097977.fastq | wc -l We can use the above trick to convert the .fastq to .fasta $ sed -n '1~4p;2~4p' SRR097977.fastq | sed 's/^@/>/g' > SRR097977.fasta Let's wrap up sed with one more use case (slightly complicated looking one). Let's say that we want capture all the transcript names from the last column (9 th column) from .gtf file. We can write something similar to , $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 | sed -E 's/.*transcript_id \"([^\"]+)\".*/\\1/' Not really what we are after as the output is as below 1 pseudogene gene 3054233 3054733 . + . gene_id \"ENSMUSG00000090025\" ; gene_name \"Gm16088\" ; gene_source \"havana\" ; gene_biotype \"pseudogene\" ; ENSMUST00000160944 ENSMUST00000160944 The is due to sed default behaviour where it prints every line, making replacements to matching lines. .i.e Some lines of the last column of Mus_musculus.GRCm38.75_chr1.gtf don't contain transcript_id . So, sed prints the entire line rather than captured group. One way to solve this would be to use grep transcript_id before sed to only work with lines containing the string transcript_id . However, sed offers a cleaner way. First, disable sed from outputting all lines with -n . Then, by appending p after the last slash sed will print all lines it\u2019s made a replacement on. The following is an illustration of -n used with p : $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 | sed -E -n 's/.*transcript_id \"([^\"]+)\".*/\\1/p' Back to homepage","title":"Find and Replace"},{"location":"5_inspectmanipulate2/","text":"Inspecting and Manipulating Text Data with Unix Tools - Part 2 \u00b6 Aho, Weinberger, Kernighan = AWK \u00b6 Awk is a scripting language used for manipulating data and generating reports. The awk command programming language requires no compiling and allows the user to use variables, numeric functions, string functions, and logical operators. Awk is a utility that enables a programmer to write tiny but effective programs in the form of statements that define text patterns that are to be searched for in each line of a document and the action that is to be taken when a match is found within a line. Awk is mostly used for pattern scanning and processing. It searches one or more files to see if they contain lines that matches with the specified patterns and then perform the associated actions. WHAT CAN WE DO WITH AWK? AWK Operations: Scans a file line by line Splits each input line into fields Compares input line/fields to pattern Performs action(s) on matched lines Useful For: Transform data files Produce formatted reports Programming Constructs: Format output lines Arithmetic and string operations Conditionals and loops Syntax : awk options 'selection_criteria {action}' input-file > output-file Options -f program-file : Reads the AWK program source from the file program-file, instead of from the first command line argument. -F fs : Use fs for the input field separator Default behaviour of awk is to print every line of data from the specified file. .i.e. mimics cat $ awk '{print}' example.bed chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 Print lines which match the given pattern $ awk '/chr1/ {print}' example.bed chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr1 10 19 awk can be used to mimic functionality of cut $ awk '{print $2 \"\\t\" $3}' example.bed 26 39 32 47 11 28 40 49 16 27 9 28 35 54 10 19 Here, we\u2019re making use of Awk\u2019s string concatenation. Two strings are concatenated if they are placed next to each other with no argument. So for each record, $2\"\\t\"$3 concatenates the second field, a tab character, and the third field. However, this is an instance where using cut is much simpler as the equivalent of above is cut -f2,3 example.bed Let\u2019s look at how we can incorporate simple pattern matching. Suppose we wanted to write a filter that only output lines where the length of the feature (end position - start position) was greater than 18. Awk supports arithmetic with the standard operators + , - , * , / , % (remainder), and ^ (exponentiation). We can subtract within a pattern to calculate the length of a feature, and filter on that expression: $ awk '$3 - $2 > 18' example.bed chr1 9 28 chr2 35 54 Comparison Description a == b a is equal to b a != b a is not equal to b a < b a is less than b a > b a is greater than b a <= b a is less than or equal to b a >= b a is greater than or equal to b a ~ b a matches regular expression pattern b a !~ b a does not match regular expression pattern b a && b logical a and b a \\|\\| b logical or a and b !a not a (logical negation) \u00b6 We can also chain patterns, by using logical operators && (AND), || (OR), and ! (NOT). For example, if we wanted all lines on chromosome 1 with a length greater than 10: $ awk '$1 ~ /chr1/ && $3 - $2 > 10' example.bed chr1 26 39 chr1 32 47 chr1 9 28 Built-In Variables and special patterns In Awk Awk\u2019s built-in variables include the field variables $1 , $2 , $3 , and so on ( $0 is the entire line) \u2014 that break a line of text into individual words or pieces called fields. NR : keeps a current count of the number of input records. Remember that records are usually lines. Awk command performs the pattern/action statements once for each record in a file. NF : keeps a count of the number of fields within the current input record. FS : contains the field separator character which is used to divide fields on the input line. The default is \u201cwhite space\u201d, meaning space and tab characters. FS can be reassigned to another character (typically in BEGIN) to change the field separator. RS : stores the current record separator character. Since, by default, an input line is the input record, the default record separator character is a newline. OFS : stores the output field separator, which separates the fields when Awk prints them. The default is a blank space. Whenever print has several parameters separated with commas, it will print the value of OFS in between each parameter. ORS : stores the output record separator, which separates the output lines when Awk prints them. The default is a newline character. print automatically outputs the contents of ORS at the end of whatever it is given to print. Also, there are two special patterns BEGIN & END BEGIN - specifies what to do before the first record is read in. Useful to initialise and set up variables END - what to do after the last record's processing is complete. Useful to print data summaries ad the end of file processing Examples We can use NR to extract ranges of lines, too; for example, if we wanted to extract all lines between 3 and 5 (inclusive): $ awk 'NR >= 3 && NR <=5' example.bed chr3 11 28 chr1 40 49 chr3 16 27 suppose we wanted to calculate the mean feature length in example.bed. We would have to take the sum feature lengths, and then divide by the total number of records. We can do this with: $ awk 'BEGIN{s = 0}; {s += ($3-$2)}; END{ print \"mean: \" s/NR};' example.bed mean: 14 Info In this example, we\u2019ve initialized a variable s to 0 in BEGIN (variables you define do not need a dollar sign). Then, for each record we increment s by the length of the feature. At the end of the records, we print this sum s divided by the number of records NR , giving the mean. awk makes it easy to convert between bioinformatics files like BED and GTF. For example, we could generate a three-column BED file from Mus_muscu\u2010 lus.GRCm38.75_chr1.gtf as follows: $ awk '!/^#/ { print $1 \"\\t\" $4-1 \"\\t\" $5}' Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 1 3054232 3054733 1 3054232 3054733 1 3054232 3054733 awk also has a very useful data structure known as an associative array. Associative arrays behave like Python\u2019s dictionaries or hashes in other languages. We can create an associative array by simply assigning a value to a key. For example, suppose we wanted to count the number of features (third column) belonging to the gene \u201cLypla1.\u201d We could do this by incrementing their values in an associative array: $ awk '/Lypla1/ {feature[$3] += 1}; END {for (k in feature) print k \"\\t\" feature[k]}' Mus_musculus.GRCm38.75_chr1.gtf CDS 56 transcript 9 start_codon 5 gene 1 exon 69 UTR 24 bioawk \u00b6 bioawk is an extension of awk , adding the support of several common biological data formats, including optionally gzip'ed BED, GFF, SAM, VCF, FASTA/Q and TAB-delimited formats with column names. It also adds a few built-in functions and an command line option to use TAB as the input/output delimiter. When the new functionality is not used, bioawk is intended to behave exactly the same as the original BWK awk. The original awk requires a YACC-compatible parser generator (e.g. Byacc or Bison). bioawk further depends on zlib so as to work with gzip'd files. YACC A parser generator is a program that takes as input a specification of a syntax, and produces as output a procedure for recognizing that language. Historically, they are also called compiler-compilers. YACC (yet another compiler-compiler) is an LALR(LookAhead, Left-to-right, Rightmost derivation producer with 1 lookahead token) parser generator. YACC was originally designed for being complemented by Lex . Lex (A Lexical Analyzer Generator) helps write programs whose control flow is directed by instances of regular expressions in the input stream. It is well suited for editor-script type transformations and for segmenting input in preparation for a parsing routine. bioawk features It can automatically recognize some popular formats and will parse different features associated with those formats. The format option is passed to bioawk using -c arg flag. Here arg can be bed, sam, vcf, gff or fastx (for both fastq and FASTA). It can also deal with other types of table formats using the -c header option. When header is specified, the field names will used for variable names, thus greatly expanding the utility.` There are several builtin functions (other than the standard awk built-ins), that are specific to biological file formats. When a format is read with bioawk , the fields get automatically parsed. You can apply several functions on these variables to get the desired output. Let\u2019s say, we read fasta format, now we have $name and $seq that holds sequence name and sequence respectively. You can use the print function ( awk builtin) to print $name and $seq . You can also use bioawk built-in with the print function to get length, reverse complement etc by just using '{print length($seq)}' . Other functions include reverse , revcomp , trimq , and , or , xor etc. Variables for each format For the -c you can either specify bed, sam, vcf, gff, fastx or header. bioawk will parse these variables for the respective format. If -c header is specified, the field names (first line) will be used as variables (spaces and special characters will be changed to under_score) bed sam vcf gff fastx chrom qname chrom seqname name start flag pos source seq end rname id feature qual name pos ref start comment score mapq alt end strand cigar qual score thickstart rnext filter filter thickend pnext info strand rgb tlen group blockcount seq attribute blocksizes qual blockstarts Back to homepage","title":"Inspecting and Manipulating Text Data with Unix Tools - Part 2"},{"location":"5_inspectmanipulate2/#inspecting-and-manipulating-text-data-with-unix-tools-part-2","text":"","title":"Inspecting and Manipulating Text Data with Unix Tools - Part 2"},{"location":"5_inspectmanipulate2/#aho-weinberger-kernighan-awk","text":"Awk is a scripting language used for manipulating data and generating reports. The awk command programming language requires no compiling and allows the user to use variables, numeric functions, string functions, and logical operators. Awk is a utility that enables a programmer to write tiny but effective programs in the form of statements that define text patterns that are to be searched for in each line of a document and the action that is to be taken when a match is found within a line. Awk is mostly used for pattern scanning and processing. It searches one or more files to see if they contain lines that matches with the specified patterns and then perform the associated actions. WHAT CAN WE DO WITH AWK? AWK Operations: Scans a file line by line Splits each input line into fields Compares input line/fields to pattern Performs action(s) on matched lines Useful For: Transform data files Produce formatted reports Programming Constructs: Format output lines Arithmetic and string operations Conditionals and loops Syntax : awk options 'selection_criteria {action}' input-file > output-file Options -f program-file : Reads the AWK program source from the file program-file, instead of from the first command line argument. -F fs : Use fs for the input field separator Default behaviour of awk is to print every line of data from the specified file. .i.e. mimics cat $ awk '{print}' example.bed chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 Print lines which match the given pattern $ awk '/chr1/ {print}' example.bed chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr1 10 19 awk can be used to mimic functionality of cut $ awk '{print $2 \"\\t\" $3}' example.bed 26 39 32 47 11 28 40 49 16 27 9 28 35 54 10 19 Here, we\u2019re making use of Awk\u2019s string concatenation. Two strings are concatenated if they are placed next to each other with no argument. So for each record, $2\"\\t\"$3 concatenates the second field, a tab character, and the third field. However, this is an instance where using cut is much simpler as the equivalent of above is cut -f2,3 example.bed Let\u2019s look at how we can incorporate simple pattern matching. Suppose we wanted to write a filter that only output lines where the length of the feature (end position - start position) was greater than 18. Awk supports arithmetic with the standard operators + , - , * , / , % (remainder), and ^ (exponentiation). We can subtract within a pattern to calculate the length of a feature, and filter on that expression: $ awk '$3 - $2 > 18' example.bed chr1 9 28 chr2 35 54 Comparison Description a == b a is equal to b a != b a is not equal to b a < b a is less than b a > b a is greater than b a <= b a is less than or equal to b a >= b a is greater than or equal to b a ~ b a matches regular expression pattern b a !~ b a does not match regular expression pattern b a && b logical a and b a \\|\\| b logical or a and b !a not a (logical negation)","title":"Aho, Weinberger, Kernighan = AWK"},{"location":"5_inspectmanipulate2/#_1","text":"We can also chain patterns, by using logical operators && (AND), || (OR), and ! (NOT). For example, if we wanted all lines on chromosome 1 with a length greater than 10: $ awk '$1 ~ /chr1/ && $3 - $2 > 10' example.bed chr1 26 39 chr1 32 47 chr1 9 28 Built-In Variables and special patterns In Awk Awk\u2019s built-in variables include the field variables $1 , $2 , $3 , and so on ( $0 is the entire line) \u2014 that break a line of text into individual words or pieces called fields. NR : keeps a current count of the number of input records. Remember that records are usually lines. Awk command performs the pattern/action statements once for each record in a file. NF : keeps a count of the number of fields within the current input record. FS : contains the field separator character which is used to divide fields on the input line. The default is \u201cwhite space\u201d, meaning space and tab characters. FS can be reassigned to another character (typically in BEGIN) to change the field separator. RS : stores the current record separator character. Since, by default, an input line is the input record, the default record separator character is a newline. OFS : stores the output field separator, which separates the fields when Awk prints them. The default is a blank space. Whenever print has several parameters separated with commas, it will print the value of OFS in between each parameter. ORS : stores the output record separator, which separates the output lines when Awk prints them. The default is a newline character. print automatically outputs the contents of ORS at the end of whatever it is given to print. Also, there are two special patterns BEGIN & END BEGIN - specifies what to do before the first record is read in. Useful to initialise and set up variables END - what to do after the last record's processing is complete. Useful to print data summaries ad the end of file processing Examples We can use NR to extract ranges of lines, too; for example, if we wanted to extract all lines between 3 and 5 (inclusive): $ awk 'NR >= 3 && NR <=5' example.bed chr3 11 28 chr1 40 49 chr3 16 27 suppose we wanted to calculate the mean feature length in example.bed. We would have to take the sum feature lengths, and then divide by the total number of records. We can do this with: $ awk 'BEGIN{s = 0}; {s += ($3-$2)}; END{ print \"mean: \" s/NR};' example.bed mean: 14 Info In this example, we\u2019ve initialized a variable s to 0 in BEGIN (variables you define do not need a dollar sign). Then, for each record we increment s by the length of the feature. At the end of the records, we print this sum s divided by the number of records NR , giving the mean. awk makes it easy to convert between bioinformatics files like BED and GTF. For example, we could generate a three-column BED file from Mus_muscu\u2010 lus.GRCm38.75_chr1.gtf as follows: $ awk '!/^#/ { print $1 \"\\t\" $4-1 \"\\t\" $5}' Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 1 3054232 3054733 1 3054232 3054733 1 3054232 3054733 awk also has a very useful data structure known as an associative array. Associative arrays behave like Python\u2019s dictionaries or hashes in other languages. We can create an associative array by simply assigning a value to a key. For example, suppose we wanted to count the number of features (third column) belonging to the gene \u201cLypla1.\u201d We could do this by incrementing their values in an associative array: $ awk '/Lypla1/ {feature[$3] += 1}; END {for (k in feature) print k \"\\t\" feature[k]}' Mus_musculus.GRCm38.75_chr1.gtf CDS 56 transcript 9 start_codon 5 gene 1 exon 69 UTR 24","title":""},{"location":"5_inspectmanipulate2/#bioawk","text":"bioawk is an extension of awk , adding the support of several common biological data formats, including optionally gzip'ed BED, GFF, SAM, VCF, FASTA/Q and TAB-delimited formats with column names. It also adds a few built-in functions and an command line option to use TAB as the input/output delimiter. When the new functionality is not used, bioawk is intended to behave exactly the same as the original BWK awk. The original awk requires a YACC-compatible parser generator (e.g. Byacc or Bison). bioawk further depends on zlib so as to work with gzip'd files. YACC A parser generator is a program that takes as input a specification of a syntax, and produces as output a procedure for recognizing that language. Historically, they are also called compiler-compilers. YACC (yet another compiler-compiler) is an LALR(LookAhead, Left-to-right, Rightmost derivation producer with 1 lookahead token) parser generator. YACC was originally designed for being complemented by Lex . Lex (A Lexical Analyzer Generator) helps write programs whose control flow is directed by instances of regular expressions in the input stream. It is well suited for editor-script type transformations and for segmenting input in preparation for a parsing routine. bioawk features It can automatically recognize some popular formats and will parse different features associated with those formats. The format option is passed to bioawk using -c arg flag. Here arg can be bed, sam, vcf, gff or fastx (for both fastq and FASTA). It can also deal with other types of table formats using the -c header option. When header is specified, the field names will used for variable names, thus greatly expanding the utility.` There are several builtin functions (other than the standard awk built-ins), that are specific to biological file formats. When a format is read with bioawk , the fields get automatically parsed. You can apply several functions on these variables to get the desired output. Let\u2019s say, we read fasta format, now we have $name and $seq that holds sequence name and sequence respectively. You can use the print function ( awk builtin) to print $name and $seq . You can also use bioawk built-in with the print function to get length, reverse complement etc by just using '{print length($seq)}' . Other functions include reverse , revcomp , trimq , and , or , xor etc. Variables for each format For the -c you can either specify bed, sam, vcf, gff, fastx or header. bioawk will parse these variables for the respective format. If -c header is specified, the field names (first line) will be used as variables (spaces and special characters will be changed to under_score) bed sam vcf gff fastx chrom qname chrom seqname name start flag pos source seq end rname id feature qual name pos ref start comment score mapq alt end strand cigar qual score thickstart rnext filter filter thickend pnext info strand rgb tlen group blockcount seq attribute blocksizes qual blockstarts Back to homepage","title":"bioawk"}]}